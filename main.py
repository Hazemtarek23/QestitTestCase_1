from datetime import datetime
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
import os
import shutil
import logging
import traceback
import asyncio
from contextlib import asynccontextmanager

# Setup logging first
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global variables for services
enhanced_test_case_service = None
test_case_service = None
doc_controller = None
risk_controller = None
expert_controller = None
test_case_controller = None
# Dynamic AI provider details
ai_provider = "Unknown"
ai_endpoint = ""

# Llama server defaults (fallbacks only; prefer env vars)
DEFAULT_AZURE_MODEL = os.getenv("LLAMA_MODEL", "unsloth.Q4_K_M.gguf")

# Display-only list (legacy). Not used by llama server.
AZURE_MODELS = []


def initialize_services():
    """Initialize services synchronously"""
    global enhanced_test_case_service, test_case_service
    global doc_controller, risk_controller, expert_controller, test_case_controller
    global ai_provider, ai_endpoint

    try:
        logger.info("Initializing services...")

        # No cloud keys needed for llama server

        # Import services after environment is set
        from services.llama_client import LlamaClient
        from services.test_case_service import EnhancedTestCaseService, TestCaseService
        from controllers.document_analysis_router import DocumentAnalysisController
        from controllers.risk_assessment_router import RiskAssessmentRouter
        from controllers.expert_analysis_router import ExpertAnalysisController
        from controllers.generate_test_cases_logic import create_test_case_controller

        current_model = DEFAULT_AZURE_MODEL

        # Create one shared generator instance to satisfy Dependency Inversion
        shared_generator = LlamaClient(DEFAULT_AZURE_MODEL)
        model_info = shared_generator.get_model_info()
        current_model = model_info.get("model_name", DEFAULT_AZURE_MODEL)
        ai_provider = model_info.get("provider", "Local LLM")
        ai_endpoint = model_info.get("endpoint", "")

        try:
            logger.info(f"Initializing with {ai_provider} model: {current_model}")

            # Create test case services
            enhanced_test_case_service = EnhancedTestCaseService(current_model, shared_generator)
            test_case_service = TestCaseService(current_model, shared_generator)

            # Create controllers
            doc_controller = DocumentAnalysisController(shared_generator)
            risk_controller = RiskAssessmentRouter(shared_generator)
            expert_controller = ExpertAnalysisController(shared_generator)
            test_case_controller = create_test_case_controller()

            logger.info(f"âœ… Successfully initialized with {ai_provider} model: {current_model}")

        except Exception as e:
            logger.error(f"Failed to initialize AI client: {str(e)}")
            raise

        # Ensure directories exist
        os.makedirs("temp", exist_ok=True)
        os.makedirs("outputs", exist_ok=True)

        logger.info("âœ… All services initialized successfully")
        return current_model

    except Exception as e:
        logger.error(f"Failed to initialize services: {str(e)}")
        return "fallback"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    # Startup
    logger.info("=" * 60)
    logger.info("AI Document Processing API v3.0 Starting Up...")
    logger.info("=" * 60)

    current_model = initialize_services()
    app.state.current_model = current_model

    logger.info(f"âœ… Provider: {ai_provider}")
    logger.info(f"âœ… Current model: {current_model}")
    logger.info(f"âœ… Endpoint: {ai_endpoint}")
    logger.info("ðŸš€ API Ready for processing!")
    logger.info("=" * 60)

    yield

    # Shutdown
    logger.info("AI Document Processing API v3.0 shutting down...")

    # Cleanup services
    try:
        if enhanced_test_case_service and hasattr(enhanced_test_case_service, 'cleanup'):
            enhanced_test_case_service.cleanup()
        logger.info("Enhanced test case service cleanup completed")
    except Exception as e:
        logger.warning(f"Error during enhanced service cleanup: {e}")

    # Clean up temporary files
    try:
        if os.path.exists("temp"):
            temp_files = os.listdir("temp")
            for temp_file in temp_files:
                try:
                    os.remove(os.path.join("temp", temp_file))
                except Exception as e:
                    logger.warning(f"Failed to remove temp file {temp_file}: {e}")
        logger.info("Temporary files cleanup completed")
    except Exception as e:
        logger.warning(f"Error during temp file cleanup: {e}")

    logger.info("âœ… Shutdown cleanup completed successfully")


# Create FastAPI app with lifespan
app = FastAPI(
    title="Enhanced AI Document Processing API (LLM-powered)",
    version="3.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --------------------- Utility Functions ------------------------

async def save_uploaded_file(uploaded_file: UploadFile) -> str:
    """Save uploaded file with proper error handling"""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Sanitize filename
        safe_filename = "".join(c for c in uploaded_file.filename if c.isalnum() or c in (' ', '.', '_')).rstrip()
        filename = f"{timestamp}_{safe_filename.replace(' ', '_')}"
        file_path = f"temp/{filename}"

        # Reset file pointer to beginning
        await uploaded_file.seek(0)
        with open(file_path, "wb") as f:
            # Read in smaller chunks to avoid memory issues
            while chunk := await uploaded_file.read(8192):
                f.write(chunk)

        # Verify file was written
        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
            raise HTTPException(status_code=500, detail="Failed to save uploaded file")

        logger.info(f"File saved at: {file_path} (size: {os.path.getsize(file_path)} bytes)")
        return file_path

    except Exception as e:
        logger.error(f"Error saving uploaded file: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save file: {str(e)}")


async def read_file_content(file_path: str) -> str:
    """Read content from various file formats"""
    if not os.path.exists(file_path):
        return ""

    try:
        # Get file extension
        _, ext = os.path.splitext(file_path.lower())

        if ext == '.pdf':
            return await read_pdf_text(file_path)
        else:
            # Handle text files
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()

    except Exception as e:
        logger.warning(f"Could not read file {file_path}: {str(e)}")
        return ""


async def read_pdf_text(file_path: str) -> str:
    """Read PDF text with better error handling"""
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"File not found: {file_path}")

    if os.path.getsize(file_path) == 0:
        raise HTTPException(status_code=400, detail=f"File is empty: {file_path}")

    try:
        import PyPDF2
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            if len(reader.pages) == 0:
                return "PDF contains no pages"

            text_parts = []
            for page_num, page in enumerate(reader.pages):
                try:
                    text = page.extract_text()
                    if text.strip():
                        text_parts.append(text)
                except Exception as page_error:
                    logger.warning(f"Failed to extract text from page {page_num}: {page_error}")
                    continue

            if not text_parts:
                return f"No readable text found in PDF: {file_path}"

            return "\n".join(text_parts)

    except ImportError:
        raise HTTPException(status_code=500, detail="PyPDF2 not installed")
    except Exception as e:
        logger.error(f"Failed to read PDF {file_path}: {e}")
        return f"Could not extract content from: {file_path}. Error: {str(e)}"


def create_sample_report():
    """Create a sample report for testing purposes"""
    try:
        current_model = getattr(app.state, 'current_model', DEFAULT_AZURE_MODEL)
        provider = getattr(app.state, 'ai_provider', 'Local LLM')
        endpoint = getattr(app.state, 'ai_endpoint', '')
        provider = getattr(app.state, 'ai_provider', 'Local LLM')
        endpoint = getattr(app.state, 'ai_endpoint', '')

        provider = getattr(app.state, 'ai_provider', 'Local LLM')
        endpoint = getattr(app.state, 'ai_endpoint', '')
        sample_content = f"""# Enhanced Sample Analysis Report ({provider} Powered)

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
AI Model: {provider} {current_model}
Endpoint: {endpoint}

## Document Analysis
This is an enhanced sample analysis report created for comprehensive testing purposes using AI models.

## Key Findings
- Feature functionality appears complete with enhanced coverage
- Requirements are well-defined and comprehensive  
- Test coverage includes advanced scenarios powered by Azure OpenAI
- Integration points identified through AI analysis
- Security considerations documented
- Performance requirements specified

## Enhanced Recommendations
- Proceed with comprehensive test case generation using the configured AI model
- Include boundary value testing with AI-generated scenarios
- Consider security and performance testing with intelligent test data
- Validate complete user experience flows
- Test integration scenarios with realistic data
- Include accessibility testing
- Validate compliance requirements

## Test Data Requirements
- Realistic test data for all scenarios generated by the configured AI model
- Multi-language support testing
- Edge case data validation
- Performance load testing data
- Security penetration testing scenarios

## AI-Powered Enhancements
- Intelligent test case generation using the configured AI model
- Natural language processing for requirement analysis
- Automated test data generation with cultural context
- Smart test coverage analysis
- Realistic banking scenario creation
"""

        sample_filename = f"enhanced_sample_analysis_azure_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        sample_path = os.path.join("outputs", sample_filename)

        with open(sample_path, 'w', encoding='utf-8') as f:
            f.write(sample_content)

        logger.info(f"Created enhanced sample report at: {sample_path}")
        return sample_path

    except Exception as e:
        logger.error(f"Error creating enhanced sample report: {e}")
        return None


# --------------------- New Workflow Route ------------------------

@app.post("/api/full-workflow")
async def full_workflow_process(
        file: UploadFile = File(...),
        user_story: str = Form(""),
        feature_title: str = Form(""),
        feature_prefix: str = Form("TC"),
        iteration_count: int = Form(300)):
    """
    Complete workflow: Upload -> Analysis -> Risk -> Expert -> Test Cases
    """
    temp_files = []
    try:
        current_model = getattr(app.state, 'current_model', DEFAULT_AZURE_MODEL)
        provider = getattr(app.state, 'ai_provider', 'Local LLM')
        endpoint = getattr(app.state, 'ai_endpoint', '')
        logger.info(f"Starting full workflow process for file: {file.filename}")

        # Step 1: Save uploaded file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        upload_path = await save_uploaded_file(file)
        temp_files.append(upload_path)

        # Step 2: Document Analysis
        logger.info("Step 1: Performing document analysis...")
        if doc_controller:
            analysis_result = await doc_controller.service.analyze_file(upload_path)
            analysis_content = await read_file_content(analysis_result)
        else:
            analysis_content = f"Document analysis for {file.filename} using {provider} model {current_model}"
            analysis_result = create_sample_report()

        # Step 3: Risk Assessment
        logger.info("Step 2: Performing risk assessment...")
        if risk_controller:
            # Create request object manually since we removed pydantic models
            risk_request = type('RiskAssessmentRequest', (), {
                'user_story': user_story,
                'analysis_text': analysis_content,
                'documentation_chunks': [analysis_content],  # Add missing attribute
                'feature_title': feature_title,  # Add missing attribute
                'analysis_type': 'comprehensive'
            })()

            risk_result = await risk_controller.service.analyze_risk(risk_request)
            risk_content = risk_result.risk_assessment if hasattr(risk_result, 'risk_assessment') else str(risk_result)
        else:
            risk_content = f"Risk assessment based on analysis using {provider} model {current_model}"

        # Save risk assessment to PDF file
        risk_filename = f"risk_assessment_{timestamp}.pdf"
        risk_path = os.path.join("outputs", risk_filename)
        
        # Import and use PDF generator
        from utils.pdf_generator import ProfessionalPDFGenerator
        pdf_generator = ProfessionalPDFGenerator()
        pdf_generator.generate_professional_report(
            title="Risk Assessment Report",
            content=risk_content,
            output_path=risk_path,
            executive_summary="Comprehensive risk assessment based on document analysis"
        )
        temp_files.append(risk_path)

        # Step 4: Expert Analysis
        logger.info("Step 3: Performing expert analysis...")
        if expert_controller:
            # Create request object manually
            expert_request = type('ExpertAnalysisRequest', (), {
                'user_story': user_story,
                'analysis_text': analysis_content,
                'risk_assessment': risk_content,
                'analysis_type': 'comprehensive'
            })()

            expert_result = await expert_controller.service.perform_expert_analysis(expert_request)
            expert_content = expert_result.expert_analysis if hasattr(expert_result, 'expert_analysis') else str(
                expert_result)
        else:
            expert_content = f"Expert analysis combining document analysis and risk assessment using {provider} model {current_model}"

        # Save expert analysis to PDF file
        expert_filename = f"expert_analysis_{timestamp}.pdf"
        expert_path = os.path.join("outputs", expert_filename)
        
        # Use PDF generator for expert analysis
        pdf_generator.generate_professional_report(
            title="Expert Analysis Report",
            content=expert_content,
            output_path=expert_path,
            executive_summary="Comprehensive expert analysis combining document analysis and risk assessment"
        )
        temp_files.append(expert_path)

        # Step 5: Generate Test Cases
        logger.info("Step 4: Generating test cases...")
        if enhanced_test_case_service:
            test_result = await enhanced_test_case_service.generate_test_cases_from_documents(
                analysis_doc=analysis_content,
                risk_doc=risk_content,
                reviewed_doc=expert_content,
                feature_prefix=feature_prefix,
                iteration_count=iteration_count
            )
        else:
            # Fallback test case generation
            test_result = {
                "success": False,
                "error": "Enhanced test case service not available"
            }

        # Prepare response
        workflow_result = {
            "status": "success",
            "message": f"Full workflow completed successfully using {provider} model: {current_model}",
            "workflow_steps": {
                "step_1_analysis": {
                    "status": "completed",
                    "file": analysis_result.replace("\\", "/") if analysis_result else None,
                    "content_preview": analysis_content[:500] + "..." if len(
                        analysis_content) > 500 else analysis_content
                },
                "step_2_risk": {
                    "status": "completed",
                    "file": risk_path.replace("\\", "/"),
                    "content_preview": risk_content[:500] + "..." if len(risk_content) > 500 else risk_content
                },
                "step_3_expert": {
                    "status": "completed",
                    "file": expert_path.replace("\\", "/"),
                    "content_preview": expert_content[:500] + "..." if len(expert_content) > 500 else expert_content
                },
                "step_4_test_cases": {
                    "status": "completed" if test_result.get("success") else "failed",
                    "generated_count": test_result.get("generated_count", 0),
                    "files": {
                        "markdown": test_result.get("markdown_path", "").replace("\\", "/") if test_result.get(
                            "markdown_path") else None,
                        "excel": test_result.get("excel_path", "").replace("\\", "/") if test_result.get(
                            "excel_path") else None
                    }
                }
            },
            "ai_model": current_model,
            "provider": provider,
            "endpoint": endpoint,
            "source_file": file.filename,
            "feature_title": feature_title,
            "user_story": user_story,
            "timestamp": timestamp
        }

        return workflow_result

    except Exception as e:
        logger.error(f"Error in full workflow process: {str(e)}")
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Full workflow failed: {str(e)}")

    finally:
        # Clean up temporary files (keep output files)
        for temp_file in temp_files:
            if temp_file.startswith("temp/") and os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                    logger.info(f"Cleaned up temporary file: {temp_file}")
                except Exception as cleanup_error:
                    logger.warning(f"Failed to clean up temporary file {temp_file}: {str(cleanup_error)}")


# --------------------- Main Routes ------------------------

@app.post("/analyze-document")
async def analyze_document(
        feature_title: str = Form(...),
        user_story: str = Form(...),
        document: UploadFile = File(...)):
    if not document.filename:
        raise HTTPException(status_code=400, detail="No file uploaded")

    file_path = None
    try:
        current_model = getattr(app.state, 'current_model', DEFAULT_AZURE_MODEL)
        provider = getattr(app.state, 'ai_provider', 'Local LLM')
        endpoint = getattr(app.state, 'ai_endpoint', '')
        file_path = await save_uploaded_file(document)

        # Use document analysis if controller is available
        if doc_controller:
            analysis_pdf_path = await doc_controller.service.analyze_file(file_path)
        else:
            # Create fallback analysis
            analysis_pdf_path = create_sample_report()

        return {
            "status": "success",
            "feature_title": feature_title,
            "user_story": user_story,
            "analysis_file": analysis_pdf_path,
            "ai_model": current_model,
            "provider": provider,
            "endpoint": endpoint
        }

    except Exception as e:
        logger.error(f"Document analysis failed: {e}")
        raise HTTPException(status_code=500, detail=f"Document analysis failed: {str(e)}")
    finally:
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except Exception as cleanup_error:
                logger.warning(f"Cleanup failed: {cleanup_error}")


@app.post("/process")
async def process_document(
        file: UploadFile = File(...),
        use_enhanced_service: bool = Form(True),
        iteration_count: int = Form(300),
        feature_prefix: str = Form("TC")):
    """Enhanced process document with large-scale comprehensive test case generation using the configured LLM."""

    try:
        current_model = getattr(app.state, 'current_model', DEFAULT_AZURE_MODEL)
        provider = getattr(app.state, 'ai_provider', 'Local LLM')
        endpoint = getattr(app.state, 'ai_endpoint', '')

        # Ensure outputs directory exists
        os.makedirs("outputs", exist_ok=True)

        # Save the uploaded file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        upload_filename = f"uploaded_document_{timestamp}_{file.filename}"
        upload_path = os.path.join("outputs", upload_filename)

        # Save uploaded file
        with open(upload_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        logger.info(f"Uploaded file saved to: {upload_path}")

        # Read file content for processing
        file_content = await read_file_content(upload_path)

        # Use Enhanced Test Case Service by default
        if use_enhanced_service and enhanced_test_case_service:
            logger.info(
                "Using Enhanced Test Case Service for large-scale comprehensive generation...")
            try:
                result = await enhanced_test_case_service.generate_large_scale_test_cases(
                    user_input=f"Generate comprehensive test cases from uploaded document: {file.filename}\n\nDocument Content Preview:\n{file_content[:2000]}...",
                    technical_analysis=f"Technical analysis extracted from file: {file.filename}",
                    domain_analysis=f"Business domain analysis from uploaded document",
                    screen_navigation=f"UI/UX flow analysis from document content",
                    test_risk=f"Risk assessment based on document requirements",
                    test_plan=f"Comprehensive test plan for {file.filename} requirements",
                    feature_prefix=feature_prefix,
                    iteration_count=iteration_count,
                    raw_input_chunk=file_content
                )

                if result["success"]:
                    test_cases_excel = result.get("excel_path")
                    test_cases_markdown = result.get("markdown_path")
                    test_case_count = result.get("generated_count", 0)
                    batches_processed = result.get("batches_processed", 1)
                    enhancement_features = result.get("enhancement_features", [])

                    logger.info(
                        f"Enhanced Service: Successfully generated {test_case_count} comprehensive test cases in {batches_processed} batches")

                    return {
                        "status": "success",
                        "service_used": "Enhanced Large-Scale Test Case Service",
                        "ai_model": current_model,
                        "provider": provider,
                        "endpoint": endpoint,
                        "test_case_count": test_case_count,
                        "batches_processed": batches_processed,
                        "files": {
                            "uploaded_document": upload_path.replace("\\", "/"),
                            "test_cases_excel": test_cases_excel.replace("\\", "/") if test_cases_excel else None,
                            "test_cases_markdown": test_cases_markdown.replace("\\",
                                                                               "/") if test_cases_markdown else None
                        },
                        "generation_details": {
                            "approach": "Enhanced Large-Scale Professional Test Case Generation",
                            "ai_model": current_model,
                            "provider": provider,
                            "endpoint": endpoint,
                            "target_count": iteration_count,
                            "actual_count": test_case_count,
                            "batches_processed": batches_processed,
                            "success": True,
                            "source_file": file.filename,
                            "timestamp": result.get("timestamp"),
                            "enhancement_features": enhancement_features
                        }
                    }
                else:
                    raise Exception(f"Enhanced service failed: {result.get('error', 'Unknown error')}")

            except Exception as enhanced_error:
                logger.warning(f"Enhanced service failed: {str(enhanced_error)}, falling back to original service")
                use_enhanced_service = False

        # Use original service (either by choice or as fallback)
        if not use_enhanced_service and test_case_service:
            logger.info("Using Original Test Case Service...")
            try:
                result = await test_case_service.generate_test_cases_from_file(
                    file_path=upload_path,
                    feature_prefix=feature_prefix,
                    iteration_count=iteration_count
                )

                if result["success"]:
                    test_cases_excel = result.get("excel_path")
                    test_cases_pdf = result.get("pdf_path")
                    test_cases_markdown = result.get("markdown_path")
                    test_case_count = result.get("generated_count", 0)

                    logger.info(f"Original Service: Successfully generated {test_case_count} test cases")

                    return {
                        "status": "success",
                        "service_used": "Original Test Case Service",
                        "ai_model": current_model,
                        "provider": provider,
                        "endpoint": endpoint,
                        "test_case_count": test_case_count,
                        "files": {
                            "uploaded_document": upload_path.replace("\\", "/"),
                            "test_cases_pdf": test_cases_pdf.replace("\\", "/") if test_cases_pdf else None,
                            "test_cases_excel": test_cases_excel.replace("\\", "/") if test_cases_excel else None,
                            "test_cases_markdown": test_cases_markdown.replace("\\",
                                                                               "/") if test_cases_markdown else None
                        },
                        "generation_details": {
                            "approach": "Original Test Case Service",
                            "ai_model": current_model,
                            "provider": provider,
                            "endpoint": endpoint,
                            "target_count": iteration_count,
                            "actual_count": test_case_count,
                            "success": True,
                            "source_file": file.filename
                        }
                    }
                else:
                    raise Exception(f"Original service failed: {result.get('error', 'Unknown error')}")

            except Exception as original_error:
                logger.error(f"Original service also failed: {str(original_error)}")

        # Final fallback - create enhanced sample test cases
        logger.info("Creating enhanced comprehensive sample test cases as final fallback")
        return {
            "status": "success",
            "service_used": "Fallback Test Case Generation",
            "ai_model": current_model,
            "provider": provider,
            "endpoint": endpoint,
            "message": "Generated fallback test cases using the configured AI model"
        }

    except Exception as main_error:
        logger.error(f"Critical error in process_document: {str(main_error)}")
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(main_error)}")


# --------------------- Basic Routes ------------------------

@app.get("/")
async def root():
    """API Information for Postman testing"""
    current_model = getattr(app.state, 'current_model', DEFAULT_AZURE_MODEL)
    provider = getattr(app.state, 'ai_provider', 'Local LLM')
    endpoint = getattr(app.state, 'ai_endpoint', '')
    return {
        "message": "Enhanced AI Document Processing API",
        "version": "3.0.0",
        "ai_provider": provider,
        "current_model": current_model,
        "endpoint": endpoint,
        "available_endpoints": [
            "POST /api/full-workflow - Complete workflow processing",
            "POST /analyze-document - Document analysis only",
            "POST /process - Test case generation only",
            "GET /api/outputs - List output files",
            "GET /api/file-list - List files by type",
            "GET /api/download/{filename} - Download specific file",
            "GET /health - Health check"
        ],
        "usage": {
            "full_workflow": "POST /api/full-workflow with file, feature_prefix, iteration_count",
            "file_download": "GET /api/download/{filename} where filename is the exact filename",
            "file_listing": "GET /api/file-list?file_type=md for markdown files, file_type=xlsx for Excel files"
        }
    }


@app.get("/health")
async def health_check():
    current_model = getattr(app.state, 'current_model', DEFAULT_AZURE_MODEL)
    provider = getattr(app.state, 'ai_provider', 'Local LLM')
    endpoint = getattr(app.state, 'ai_endpoint', '')
    return {
        "status": "healthy",
        "service": "Enhanced AI Document Processing API",
        "version": "3.0.0",
        "ai_provider": provider,
        "current_model": current_model,
        "endpoint": endpoint,
        "services": {
            "enhanced_test_case_service": "connected - Large Scale Generation Ready",
            "original_test_case_service": "connected - Fallback Ready"
        },
        "controllers": {
            "document_analysis": "connected" if doc_controller else "fallback",
            "risk_assessment": "connected" if risk_controller else "fallback",
            "expert_analysis": "connected" if expert_controller else "fallback",
            "test_case": "connected" if test_case_controller else "fallback"
        },
        "model_info": {
            "provider": provider,
            "current_model": current_model,
            "endpoint": endpoint,
            "available_models": AZURE_MODELS
        }
    }


# --------------------- File Management Routes ------------------------

@app.get("/api/outputs")
async def list_outputs():
    """List files in outputs directory"""
    try:
        outputs_dir = "outputs"
        if not os.path.exists(outputs_dir):
            os.makedirs(outputs_dir, exist_ok=True)
            return {"files": []}

        files = []
        for filename in os.listdir(outputs_dir):
            file_path = os.path.join(outputs_dir, filename)
            if os.path.isfile(file_path):
                files.append({
                    "filename": filename,
                    "path": file_path.replace("\\", "/"),
                    "size": os.path.getsize(file_path),
                    "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
                })

        return {"files": files}

    except Exception as e:
        logger.error(f"Error listing output files: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list output files: {str(e)}")


@app.get("/api/file-list")
async def file_list(file_type: str = None):
    """List files by type (md, xlsx, pdf, txt) or all files if no type specified"""
    try:
        outputs_dir = "outputs"
        if not os.path.exists(outputs_dir):
            os.makedirs(outputs_dir, exist_ok=True)
            return {"files": []}

        files = []
        for filename in os.listdir(outputs_dir):
            # Filter by file type if specified
            if file_type:
                if not filename.lower().endswith(f'.{file_type.lower()}'):
                    continue
            
            file_path = os.path.join(outputs_dir, filename)
            if os.path.isfile(file_path):
                files.append({
                    "filename": filename,
                    "path": file_path.replace("\\", "/"),
                    "size": os.path.getsize(file_path),
                    "modified": datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
                })

        return {"files": files}

    except Exception as e:
        logger.error(f"Error listing files by type: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list files: {str(e)}")


@app.post("/api/generate-test-cases")
async def generate_test_cases_from_prompt(request: Request):
    """Generate test cases using a provided system prompt and context, return markdown file path and download URL"""
    try:
        data = await request.json()

        # Inputs
        system_prompt_input = data.get("system_prompt") or "You are a test case generation specialist. Generate detailed test cases in markdown."
        user_input = data.get("userInput", "")
        technical_analysis = data.get("technicalAnalysisResults", "")
        domain_analysis = data.get("domainAnalysisResults", "")
        screen_navigation = data.get("screenNavigation", "")
        test_risk = data.get("testRisk", "")
        test_plan = data.get("testPlan", "")
        raw_input_chunk = data.get("rawInputChunk", "")
        feature_prefix = data.get("feature_prefix", "TC")
        iteration_count = int(data.get("iteration_count", 50))

        # Safely substitute only known placeholders to avoid KeyError on other braces
        system_prompt = (
            system_prompt_input
            .replace("{iteration_count}", str(iteration_count))
            .replace("{feature_prefix}", feature_prefix)
        )

        user_prompt = f"""User story:
{user_input}

Based on:
Technical Analysis: {technical_analysis}
Domain Expertise Analysis: {domain_analysis}
Screen Navigation: {screen_navigation}
Test Risk Analysis: {test_risk}
Test Plan: {test_plan}

Create {iteration_count} test cases following the exact markdown structure provided in the system prompt.

IMPORTANT: Each test case must be completely different from any other test cases. Focus on unique aspects or scenarios of the feature that haven't been covered in previous test cases.
Use any raw input context if needed:
{raw_input_chunk}
"""

        # Use the shared generator
        if not enhanced_test_case_service or not hasattr(enhanced_test_case_service, "azure_client"):
            raise HTTPException(status_code=500, detail="Test case service not initialized")

        content = await enhanced_test_case_service.azure_client.generate_text(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_length=4096
        )

        if not content:
            raise HTTPException(status_code=500, detail="Failed to generate test cases")

        # Save to outputs
        os.makedirs("outputs", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        md_filename = f"Strict_Test_Cases_{feature_prefix}_{timestamp}.md"
        md_path = os.path.join("outputs", md_filename)
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(content)

        download_url = f"/api/download/outputs/{md_filename}"

        return {
            "status": "success",
            "message": "Test cases generated successfully",
            "file": md_path.replace("\\", "/"),
            "download_url": download_url,
            "iteration_count": iteration_count,
            "feature_prefix": feature_prefix
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating test cases from prompt: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to generate test cases: {str(e)}")


@app.get("/api/test-download")
async def test_download():
    """Test endpoint to show available files and their download URLs"""
    try:
        outputs_dir = "outputs"
        if not os.path.exists(outputs_dir):
            return {"message": "Outputs directory does not exist", "files": []}

        files = []
        for filename in os.listdir(outputs_dir):
            if os.path.isfile(os.path.join(outputs_dir, filename)):
                download_url = f"/api/download/{filename}"
                files.append({
                    "filename": filename,
                    "download_url": download_url,
                    "full_url": f"http://localhost:8000{download_url}",
                    "size": os.path.getsize(os.path.join(outputs_dir, filename)),
                    "type": os.path.splitext(filename)[1].lower()
                })

        return {
            "message": "Available files for download",
            "files": files,
            "instructions": "Use GET request to /api/download/{filename} to download files"
        }

    except Exception as e:
        logger.error(f"Error in test download endpoint: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list files: {str(e)}")


@app.get("/api/download/{filename:path}")
@app.post("/api/download/{filename:path}")
@app.head("/api/download/{filename:path}")
async def download_file(filename: str, request: Request):
    """Download a file - checks outputs directory first, then temp directory"""
    try:
        # Log request details for debugging
        logger.info(f"Download request - Method: {request.method}, Filename: {filename}")
        logger.info(f"Request headers: {dict(request.headers)}")
        logger.info(f"Request URL: {request.url}")
        
        # Sanitize filename to prevent directory traversal
        safe_filename = os.path.basename(filename)
        logger.info(f"Download request for filename: '{filename}' -> sanitized to: '{safe_filename}'")

        # Check outputs directory first (for test cases)
        outputs_path = os.path.join("outputs", safe_filename)
        if os.path.exists(outputs_path):
            file_path = outputs_path
            logger.info(f"Found file in outputs: {file_path}")
        else:
            # Check temp directory
            temp_path = os.path.join("temp", safe_filename)
            if os.path.exists(temp_path):
                file_path = temp_path
                logger.info(f"Found file in temp: {file_path}")
            else:
                logger.error(f"File not found in either directory: {safe_filename}")
                logger.error(f"Checked paths: {outputs_path}, {temp_path}")
                logger.error(f"Outputs directory contents: {os.listdir('outputs') if os.path.exists('outputs') else 'Directory does not exist'}")
                logger.error(f"Temp directory contents: {os.listdir('temp') if os.path.exists('temp') else 'Directory does not exist'}")
                raise HTTPException(status_code=404, detail=f"File not found: {filename}")

        # Determine media type based on extension
        _, ext = os.path.splitext(filename.lower())
        media_type_map = {
            '.pdf': 'application/pdf',
            '.txt': 'text/plain',
            '.md': 'text/markdown',
            '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            '.csv': 'text/csv',
            '.xls': 'application/vnd.ms-excel'
        }
        media_type = media_type_map.get(ext, 'application/octet-stream')
        
        logger.info(f"Serving file: {file_path} with media type: {media_type}")

        return FileResponse(
            path=file_path,
            filename=safe_filename,
            media_type=media_type
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error downloading file {filename}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to download file: {str(e)}")


# --------------------- Main Application Entry Point ------------------------

if __name__ == "__main__":
    import uvicorn

    # Log comprehensive startup information
    logger.info("=" * 70)
    logger.info("AI DOCUMENT PROCESSING API v3.0")
    logger.info("Enhanced Large-Scale Test Case Generation System")
    logger.info(f"ðŸš€ POWERED BY {ai_provider or 'Local LLM'}")
    logger.info("=" * 70)
    logger.info(f"ðŸ¤– AI Provider: {ai_provider or 'Local LLM'}")
    logger.info(f"ðŸ§  Default Model: {DEFAULT_AZURE_MODEL}")
    logger.info(f"ðŸ”— Endpoint: {ai_endpoint}")
    logger.info("=" * 70)
    logger.info("Enhanced Features:")
    logger.info("  ðŸš€ Large-scale test case generation (300+ cases)")
    logger.info("  ðŸ“ Professional banking test scenarios")
    logger.info("  ðŸ’¾ Advanced Excel output with formatting")
    logger.info("  ðŸ”„ Batch processing for scalability")
    logger.info("  ðŸ“Š Comprehensive test coverage analysis")
    logger.info("  ðŸ—ï¸ Realistic banking product test data")
    logger.info("  ðŸ”— End-to-end business process testing")
    logger.info("  ðŸ¤– AI natural language processing")
    logger.info("  ðŸ§  AI-powered intelligent test generation")
    logger.info("  ðŸŒ Multi-language and cultural context support")
    logger.info("  ðŸ”„ Full workflow processing (Analysis -> Risk -> Expert -> Test Cases)")
    logger.info("=" * 70)

    uvicorn.run(app, host="0.0.0.0", port=8000)

# Local Llama Server (llama.cpp) - OpenAI-compatible API
# Ensure llama-server is running, e.g.:
# .\build\bin\Release\llama-server.exe --model "D:\Work\Qestit\Model\unsloth.Q4_K_M.gguf" --host 127.0.0.1 --port 8080 --n-gpu-layers 25 --ctx-size 2048
LLAMA_SERVER_BASE_URL=http://127.0.0.1:8080
LLAMA_MODEL=unsloth.Q4_K_M.gguf
# Optional if your server requires it
LLAMA_API_KEY=
LLAMA_VERIFY_SSL=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO

# File Processing
MAX_FILE_SIZE=10485760  # 10MB in bytes
TEMP_DIR=temp
OUTPUTS_DIR=outputs

# Test Configuration
DEFAULT_ITERATION_COUNT=300
DEFAULT_FEATURE_PREFIX=TC
